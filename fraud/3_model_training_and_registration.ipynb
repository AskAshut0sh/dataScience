{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbdfd41",
   "metadata": {},
   "source": [
    "## Model training and registration\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "- Load a training dataset from the feature store\n",
    "- Train a model on the dataset\n",
    "- Register the model in the model registry.\n",
    "\n",
    "This will introduce a new library, `hsml`, which contains functionality to keep track of models and deploy them.\n",
    "\n",
    "We will train the model using standard Python and Scikit-learn, although it could just as well be trained with other machine learning frameworks such as PySpark, TensorFlow and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3b9bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "import pandas as pd\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5847a",
   "metadata": {},
   "source": [
    "### Load training data from feature store\n",
    "\n",
    "First, we'll need to fetch the training dataset that we created in the previous notebook.\n",
    "\n",
    "<!-- As you might remember, the feature store has mutable feature groups and immutable training datasets. The feature groups can get continuously updated, but a training dataset is \"frozen\" once created, including potential training validation splits.   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebec6329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: pyarrow.hdfs.HadoopFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n"
     ]
    }
   ],
   "source": [
    "td = fs.get_training_dataset(\"transactions_dataset_splitted\", version=1)\n",
    "train_df = td.read('train')\n",
    "val_df = td.read('validation')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf1f62",
   "metadata": {},
   "source": [
    "The next cell is just a type check to make sure we have Pandas data frames. The reason for the check is that if this notebook ever gets executed as a Hopsworks Job, it may be executed with a PySpark kernel and `train_df` and `val_df` would be PySpark data frames at this point, which would not work when training a Scikit-learn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42fa63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove??\n",
    "\n",
    "if not type(train_df) == pd.core.frame.DataFrame: \n",
    "    train_df = train_df.toPandas()\n",
    "    val_df = val_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24436cd",
   "metadata": {},
   "source": [
    "We will train a model to predict `fraud_label` given the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e05dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'fraud_label'\n",
    "features = list(set(train_df.columns) - set([target]))\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_val, y_val = val_df[features], val_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b84b1d0",
   "metadata": {},
   "source": [
    "Let's check the distribution of our target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4895016",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a0a96",
   "metadata": {},
   "source": [
    "Notice that the distribution is extremely skewed, which is natural considering that fraudulent transactions make up a tiny part of all transactions. Thus we should somehow address the class imbalance. There are many approaches for this, such as weighting the loss function, over- or undersampling, creating synthetic data or modifying the decision threshold. In this example, we'll use the simplest method which is to just supply a class weight parameter to our learning algorithm. The class weight will affect how much importance is attached to each class, which in our case means that a higher importance will be placed on positive (fraudulent) samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf4de6",
   "metadata": {},
   "source": [
    "### Train Logistic Regression Model\n",
    "\n",
    "We will train a simple logistic regression model on the training split and assess performance on the validation split. Here, the focus will not be on training a good model; there are many ways to try to train one that has better performance than the one shown here. The emphasis is rather on showing how to train models and track them in the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b02ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set class weights in proportion to the label ratio.\n",
    "class_weight = 'balanced'\n",
    "\n",
    "clf = LogisticRegression(class_weight=class_weight, solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38143e4b",
   "metadata": {},
   "source": [
    "<!-- Another option is to manually specify numeric weights for each label by ourselves. -->\n",
    "\n",
    "Let's go ahead and fit the model on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55cb337c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfeb4d1",
   "metadata": {},
   "source": [
    "Next we evaluate the model performance on the validation data. Since our dataset is unbalanced we will use *precision* and *recall* as evaluation metrics rather than *accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e23e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95    208437\n",
      "           1       0.02      0.80      0.03       413\n",
      "\n",
      "    accuracy                           0.90    208850\n",
      "   macro avg       0.51      0.85      0.49    208850\n",
      "weighted avg       1.00      0.90      0.95    208850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "preds = clf.predict(X_val)\n",
    "pos_precision = precision_score(y_true=y_val, y_pred=preds)\n",
    "pos_recall = recall_score(y_true=y_val, y_pred=preds)\n",
    "\n",
    "print(classification_report(y_true=y_val, y_pred=preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d91d9",
   "metadata": {},
   "source": [
    "We can see that our model performs poorly on the positive (fraud) class, which is the one we are mostly concerned about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd005ae",
   "metadata": {},
   "source": [
    "### Register model\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where we can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints.\n",
    "\n",
    "Let's connect to the model registry using the [HSML library](https://docs.hopsworks.ai/machine-learning-api/latest) from Hopsworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d5fb22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f7260",
   "metadata": {},
   "source": [
    "Before registering the model we will export it as a pickle file using joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "os.mkdir('tmp_model')\n",
    "joblib.dump(clf, 'tmp_model/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb793fa",
   "metadata": {},
   "source": [
    "The model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/machine-learning-api/latest/generated/model_schema/), which describes the inputs and outputs for a model. In our case, we need to \n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5f95cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82be0e1aedc34f94ad9aae36da358dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exported model fraud_tutorial_model with version 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hsml.sklearn.model.Model at 0x7f1f48189c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "input_schema = Schema(X_train)\n",
    "output_schema = Schema(y_train)\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40853df1",
   "metadata": {},
   "source": [
    "With the schema in place we can finally register our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68144e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mr.sklearn.create_model(\"fraud_tutorial_model\", \n",
    "                                metrics={'positive_precision': pos_precision, 'positive_recall': pos_recall},\n",
    "                                input_example=X_train,\n",
    "                                model_schema=model_schema)\n",
    "\n",
    "model.save('tmp_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79230b1",
   "metadata": {},
   "source": [
    "It's important to know that every time you save a model with the same name, a new version of the model will be saved, so nothing will be overwritten. In this way, you can compare several versions of the same model - or create a model with a new name, if you prefer that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b077e",
   "metadata": {},
   "source": [
    "#### Finding the best performing model\n",
    "\n",
    "Let's imagine you have trained and registered several versions of the same model. Now you can query the model registry for the best model according to your preferred criterion, say positive recall in our case.\n",
    "\n",
    "The `direction` option is used to indicate if the metric should be high or low (max or min); in our case it should be high (max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11960427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'fraud_tutorial_model_1',\n",
       " 'experimentId': None,\n",
       " 'projectName': 'clean_up',\n",
       " 'experimentProjectName': 'clean_up',\n",
       " 'name': 'fraud_tutorial_model',\n",
       " 'modelSchema': {'href': 'https://hopsworks.glassfish.service.consul:8182/hopsworks-api/api/project/125/dataset/Projects/clean_up/Models/fraud_tutorial_model/1/model_schema.json',\n",
       "  'zip_state': 'NONE'},\n",
       " 'version': 1,\n",
       " 'description': 'A collection of models for fraud_tutorial_model',\n",
       " 'inputExample': {'href': 'https://hopsworks.glassfish.service.consul:8182/hopsworks-api/api/project/125/dataset/Projects/clean_up/Models/fraud_tutorial_model/1/input_example.json',\n",
       "  'zip_state': 'NONE'},\n",
       " 'framework': 'SKLEARN',\n",
       " 'metrics': {'positive_precision': '0.016508378499328725',\n",
       "  'positive_recall': '0.8038740920096852'},\n",
       " 'trainingDataset': None,\n",
       " 'environment': ['/Projects/clean_up/Models/fraud_tutorial_model/1/environment.yml'],\n",
       " 'program': 'Models/fraud_tutorial_model/1/program.ipynb'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = mr.get_best_model(name=\"fraud_tutorial_model\", metric=\"positive_recall\", direction=\"max\")\n",
    "best_model.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a25f21",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll look at hyperparameter optimization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
