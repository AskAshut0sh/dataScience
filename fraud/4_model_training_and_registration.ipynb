{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c673bac-e4ea-46e5-86d7-ed1c17ea296c",
   "metadata": {},
   "source": [
    "# Model training and registration\n",
    "\n",
    "In this notebook we will\n",
    "\n",
    "- Load a training dataset from the feature store\n",
    "- Train a model\n",
    "- Register the model in the model registry.\n",
    "\n",
    "This will introduce a new library, hsml, which contains functionality to keep track of models and deploy them.\n",
    "\n",
    "In this notebook, we will train a model using standard Python and Scikit-learn. It could also have been done with e.g. PySpark, Tensorflow or PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be1d124-472d-49f9-8b92-ff5ccdd2a7f3",
   "metadata": {},
   "source": [
    "## (1) Load training data from feature store\n",
    "\n",
    "First, we need to interface with our project's feature store and fetch the training dataset that we created in the previous step of the tutorial. As you might remember, the feature store has mutable feature groups and immutable training datasets. The feature groups can get continuously updated, but a training dataset is \"frozen\" once created, including potential training validation splits.  \n",
    "\n",
    "Start by connecting to the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b57be9d-3e62-4e7c-b263-6dcac94a68a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "import pandas as pd\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a97df01-8c86-445c-96a5-31e5d432af59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: pyarrow.hdfs.HadoopFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n"
     ]
    }
   ],
   "source": [
    "td = fs.get_training_dataset(\"transactions_dataset_splitted\", version=2)\n",
    "train_df = td.read('train')\n",
    "val_df = td.read('validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13440716-9b3c-4a21-b1f4-bd7002582f47",
   "metadata": {},
   "source": [
    "Look briefly at the data to make sure everything looks all right (in particular, we should not have any categorical features left here, only numercial features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7a247-737b-4784-9c00-a5bf4f090ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f110d8-45a9-4fc9-b042-32877b6da738",
   "metadata": {},
   "source": [
    "## (2) Train logistic regression model\n",
    "\n",
    "Here we will train a predictive model on the training split and assess performance on the validation split. The focus will not be on training a good model; there are many ways to try to train one that has better performance than the one shown here. The emphasis is rather on showing how to train models and track them in the model registry.\n",
    "\n",
    "With that being said, notice that the dataset is very skewed in terms of outcome, which is natural considering that fraudulent transactions make up a tiny part of all transactions. Thus we should somehow address the class imbalance. There are many approaches for this, such as weighting the loss function, over- or undersampling, creating synthetic data or modifying the decision threshold. In this example, we'll use the simplest method which is to just supply a class weight parameter to our learning algorithm. The class weight will affect how much importance is attached to each class.\n",
    "\n",
    "The `class_weight` parameter can either be set to \"balanced\", in which case the weights will just be set in proportion to the ration of labels, or we can specify numeric weights for each label by ourselves. Let's first use the \"balanced\" option.\n",
    "\n",
    "When we save the trained model, we'll also want to attach a performance metric to it. In this scenario with unbalanced classes, it is more useful to look at precision and recall rather than accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fed8b98a-e3eb-42a9-bf4a-89def5b5737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65481a-9b31-41e5-84d2-d05b9ffb598f",
   "metadata": {},
   "source": [
    "The next cell is just a type check to make sure we have Pandas data frames. The reason for the check is that if this notebook ever gets executed as a Hopsworks Job, it may be executed with a PySpark kernel and `train_df` and `val_df` would be PySpark data frames at this point, which would not work with the `clf.fit()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cf4de3-bfbf-4939-a0fc-6bd1e1f365b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not type(train_df) == pd.core.frame.DataFrame: \n",
    "    train_df = train_df.toPandas()\n",
    "    val_df = val_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1f00a-c0b2-4d7e-ab06-0f2d3bae1fcf",
   "metadata": {},
   "source": [
    "Separate the predictive features from the label to prepare for model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638ea5bf-ce9f-4d9a-86d1-6e303ea927bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'fraud_label'\n",
    "features = list(set(train_df.columns) - set([target]))\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_val, y_val = val_df[features], val_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32677f47-31a6-4b8f-a602-67efa14ad3eb",
   "metadata": {},
   "source": [
    "Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccdb123f-a391-4b6f-b1f5-4ec16d2cedde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "# clf = RandomForestClassifier(class_weight='balanced', n_estimators=500)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751eac0-001a-4447-beb4-1de433ad08a4",
   "metadata": {},
   "source": [
    "Evaluate model performance on the validation data. \n",
    "\n",
    "For human consumption, a `classification_report` is nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1623a89-743d-4c00-b49b-34a55cb61b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.57      0.72    207677\n",
      "           1       0.00      0.85      0.01       456\n",
      "\n",
      "    accuracy                           0.57    208133\n",
      "   macro avg       0.50      0.71      0.37    208133\n",
      "weighted avg       1.00      0.57      0.72    208133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_true=y_val, y_pred=preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fadbbb-104d-4777-b041-c9847e5b80a3",
   "metadata": {},
   "source": [
    "For recording model performance in registered models, we can define some useful metrics for this particular problem. \n",
    "\n",
    "Since we are mostly interested in the rare positive class (fraud, i.e. label 1), the precision score (# true positives / # predicted positives) for the positive class seems like a good metric. Let's also use the recall for the same class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9561dc9-91d0-4aff-9fb3-a465b67f566c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud class precision: 0.004303430575376412, recall: 0.8530701754385965\n"
     ]
    }
   ],
   "source": [
    "pos_precision = precision_score(y_true=y_val, y_pred=preds, pos_label=1)\n",
    "pos_recall = recall_score(y_true=y_val, y_pred=preds, pos_label=1)\n",
    "\n",
    "print(f'Fraud class precision: {pos_precision}, recall: {pos_recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549889d-4a15-406e-b0f0-390e1d44c10a",
   "metadata": {},
   "source": [
    "## (3) Register model\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where we can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints.\n",
    "\n",
    "In order to talk to the model registry, we need to use the HSML library from Hopsworks. It should be pre-installed if you work through this tutorial in a Hopsworks Jupyter session, otherwise it is easy to pip install.\n",
    "\n",
    "Let's connect to the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "957ced42-31cc-4891-9061-93916e5fc4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "conn = hsml.connection()\n",
    "\n",
    "mr = conn.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f4dae-cb63-4be0-bb66-26075a275e08",
   "metadata": {},
   "source": [
    "To prepare for registering the model, we will export the classifier as a pickle file using joblib and then save it as a model. The model needs to be set up with a Schema, but this can be obtained automatically from training examples, as shown below.\n",
    "\n",
    "It's important to know that every time you save a model with the same name, a new version of the model will be saved, so nothing will be overwritten. In this way, you can compare several versions of the same model - or create a model with a new name, if you prefer that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0e738ab-8097-40ab-8f60-0e2658a0dee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f01d53fd224547975d7c4c8fe041f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exported model fraud_tutorial_model with version 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hsml.sklearn.model.Model at 0x7f50a5dcce80>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "os.mkdir('tmp_model')\n",
    "joblib.dump(clf, 'tmp_model/model.pkl')\n",
    "\n",
    "MODEL_NAME = \"fraud_tutorial_model\"\n",
    "\n",
    "input_schema = Schema(X_train)\n",
    "output_schema = Schema(y_train)\n",
    "\n",
    "model = mr.sklearn.create_model(MODEL_NAME, \n",
    "                                metrics={'positive_precision': pos_precision, 'positive_recall': pos_recall},\n",
    "                                input_example=X_train,\n",
    "                                model_schema=ModelSchema(input_schema=input_schema, output_schema=output_schema))\n",
    "\n",
    "model.save('tmp_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0385d1-89f0-4028-99d0-bc00e2c5ebfd",
   "metadata": {},
   "source": [
    "### Finding the best performing model\n",
    "\n",
    "Let's imagine you have trained and registered several versions of the same model. Now you can query the model registry for the best model according to your preferred criterion, say positive recall in our case.\n",
    "\n",
    "The `direction` option is used to indicate if the metric should be high or low (max or min); in our case it should be high (max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17675525-89ae-49ff-b2af-0b151fd3b41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'fraud_tutorial_model_1',\n",
       " 'experimentId': None,\n",
       " 'projectName': 'fraud_tutorial',\n",
       " 'experimentProjectName': 'fraud_tutorial',\n",
       " 'name': 'fraud_tutorial_model',\n",
       " 'modelSchema': {'href': 'https://hopsworks.glassfish.service.consul:8182/hopsworks-api/api/project/120/dataset/Projects/fraud_tutorial/Models/fraud_tutorial_model/1/model_schema.json',\n",
       "  'zip_state': 'NONE'},\n",
       " 'version': 1,\n",
       " 'description': 'A collection of models for fraud_tutorial_model',\n",
       " 'inputExample': {'href': 'https://hopsworks.glassfish.service.consul:8182/hopsworks-api/api/project/120/dataset/Projects/fraud_tutorial/Models/fraud_tutorial_model/1/input_example.json',\n",
       "  'zip_state': 'NONE'},\n",
       " 'framework': 'SKLEARN',\n",
       " 'metrics': {'positive_precision': '0.004303430575376412',\n",
       "  'positive_recall': '0.8530701754385965'},\n",
       " 'trainingDataset': None,\n",
       " 'environment': ['/Projects/fraud_tutorial/Models/fraud_tutorial_model/1/environment.yml'],\n",
       " 'program': 'Models/fraud_tutorial_model/1/program.ipynb'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = mr.get_best_model(name=\"fraud_tutorial_model\", metric=\"positive_recall\", direction=\"max\")\n",
    "best_model.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58facab-dec6-481b-aea2-d374e8018259",
   "metadata": {},
   "source": [
    "## Next chapter\n",
    "\n",
    "In the next chapter, we'll look at hyperparameter optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fabd4f-2b09-49a7-83cb-1905d260dbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
