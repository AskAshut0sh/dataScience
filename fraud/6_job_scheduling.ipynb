{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc3c712-6527-4cef-91e4-8127b3942032",
   "metadata": {},
   "source": [
    "## Job scheduling with Airflow\n",
    "\n",
    "In this notebook, you'll learn how to automate Hopsworks jobs with the built-in Airflow integration. This can be useful if you have tasks that need to be repeated with a certain time interval, such as ingesting and preprocessing data or training a new model.\n",
    "\n",
    "An [Airflow DAG](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html) (directed acyclic graph) describes how different steps in a workflow are connected. In this notebook we'll create a simple DAG for illustrative purposes which will run the first two notebooks in this tutorial series:\n",
    "\n",
    "1. Load data and do feature engineering.\n",
    "2. Create a dataset from the data.\n",
    "\n",
    "If you are unfamiliar with Apache Airflow it could be a good idea to read up on the [concepts](https://airflow.apache.org/docs/apache-airflow/stable/concepts/index.html) (in particular *DAG*, *Task*, and *Operator*) before continuing with this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af7ddb6-e0e1-4223-abf9-e7d56d6f8f09",
   "metadata": {},
   "source": [
    "### Creating Jobs\n",
    "\n",
    "First, we'll need to create the jobs that corresponds to the tasks we will run. A job can be created either programmatically, or in the Hopsworks UI (as described in the [documentation](https://docs.hopsworks.ai/hopsworks/latest/compute/jobs/)). You can create four types of jobs: Spark, Flink, Python, and Docker. However, the latter two are only available in the Enterprise Edition of Hopsworks. In this example, we'll simply convert the first two notebooks of this tutorial series into Spark Jobs.\n",
    "\n",
    "Inside your project, go to `Jobs => New Job`. Here you need to give the job a name and select a file that contains code for it. You can either upload a file or select a file from your Hopsworks cluster. If you've worked with notebooks on your Hopsworks cluster you would click on `From Project => Jupyter` and then select the notebook you want to convert to a job. Do this with the first two notebooks and name them `feature_group_job` and `dataset_job`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb50d3",
   "metadata": {},
   "source": [
    "### Creating a DAG\n",
    "\n",
    "#### Hopsworks Operators\n",
    "We'll use the *HopsworksLaunchOperator*, which will launch the jobs that we just created, and the *HopsworksJobSuccessSensor*, which will tell us whether a job has succeeded or not. The control flow will look like this:\n",
    "\n",
    "1. Launch feature engineering job using a *HopsworksLaunchOperator*.\n",
    "2. Check that the job was successfully completed using a *HopsworksJobSuccessSensor*.\n",
    "3. Launch dataset creation job using a *HopsworksLaunchOperator*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1903456f",
   "metadata": {},
   "source": [
    "#### DAG Definition File\n",
    "\n",
    "Next, we'll create the DAG definition file. We'll schedule the DAG to run at 04:00 every day in this example.\n",
    "\n",
    "You will need to fill in your Hopsworks username and project name in the code below (see comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79107fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dag.py\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "\n",
    "from hopsworks_plugin.operators.hopsworks_operator import HopsworksLaunchOperator\n",
    "from hopsworks_plugin.sensors.hopsworks_sensor import HopsworksJobSuccessSensor\n",
    "\n",
    "HOPSWORKS_USERNAME = \"\" # TODO: Change to your username in Hopsworks.\n",
    "PROJECT_NAME = \"\" # TODO: Change to your project ID.\n",
    "\n",
    "args = {\n",
    "    \"owner\": HOPSWORKS_USERNAME,\n",
    "    \"depends_on_past\": False,\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id = \"fraud_dag\",\n",
    "    default_args = args,\n",
    "    # The start date is arbitrary as we set catchup = False.\n",
    "    start_date = datetime(2022,1,1),\n",
    "    catchup = False,\n",
    "    schedule_interval = \"0 4 * * */1\"\n",
    ")\n",
    "\n",
    "task1 = HopsworksLaunchOperator(dag=dag, task_id=\"run_job_0\", job_name=\"feature_group_job\", project_name=PROJECT_NAME)\n",
    "task2 = HopsworksLaunchOperator(dag=dag, task_id=\"run_job_1\", job_name=\"dataset_job\", project_name=PROJECT_NAME)\n",
    "\n",
    "sensor = HopsworksJobSuccessSensor(dag=dag,\n",
    "                                   poke_interval=10,\n",
    "                                   task_id=\"wait_for_success_job_0\",\n",
    "                                   job_name=\"feature_group_job\",\n",
    "                                   project_name=PROJECT_NAME)\n",
    "\n",
    "task1 >> sensor >> task2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ff9d7",
   "metadata": {},
   "source": [
    "Running this code will create a file called `dag.py`. Go to `Airflow` in the Hopsworks UI, click on the three dots in the top right bar, and click on `upload_files` to upload it. Next, you will need to click on `Open Airflow`, which should show you a list of the DAGs you have uploaded. Switch from `OFF` to `ON` on the left-hand side if you want to activate your DAG. Needless to say, make sure you switch back to `OFF` again so that the DAG doesn't run every day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
