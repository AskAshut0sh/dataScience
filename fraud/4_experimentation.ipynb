{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef9a856",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization with Maggy\n",
    "\n",
    "*Note: this notebook needs to be run with a PySpark kernel to work properly!*\n",
    "\n",
    "We'll use the [Maggy](https://maggy.ai/master/) library from Hopsworks to run experiments with hyperparameter tuning. \n",
    "\n",
    "The way it works is that we wrap training code in a function that we feed to an experiment object that executes the code. We can use code similar to the previous notebook, `4_model_training_and_registration.ipynb`.\n",
    "\n",
    "The function will accept input parameters corresponding to model parameters that we want to tune. In this tutorial, a simple optimization we might want to do is to find a suitable value for the class weight on the positive class. With another model, for instance random forest or gradient boosting, you might want to tune the number of trees in the ensemble, the maximum depth of the trees, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddaff18",
   "metadata": {},
   "source": [
    "Start by reading the training and validation data in the same way as for the previous notebook. We give the code without comments for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821ba2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "import pandas as pd\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()\n",
    "td = fs.get_training_dataset(\"transactions_dataset_splitted\", version=1)\n",
    "train_df = td.read('train')\n",
    "val_df = td.read('validation')\n",
    "\n",
    "if not type(train_df) == pd.core.frame.DataFrame: \n",
    "    train_df = train_df.toPandas()\n",
    "    val_df = val_df.toPandas()\n",
    "    \n",
    "target = 'fraud_label'\n",
    "features = list(set(train_df.columns) - set([target]))\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_val, y_val = val_df[features], val_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5c1a6",
   "metadata": {},
   "source": [
    "Now wrap the training code into a function that returns some metric we want to optimize. \n",
    "\n",
    "This code assumes that the `X_train`, `y_train` etc variables already exist in the namespace. You could also require them to be inputs to the `fraud_logreg_train` function but would then need to modify how you call the function in the last cell for example using Python's `partial` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231c61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_logreg_train(class_weight):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_score\n",
    "    clf = LogisticRegression(class_weight={0: 1, 1: class_weight}, solver='liblinear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_val)\n",
    "    pos_prec = precision_score(y_true=y_val, y_pred=preds, pos_label=1)\n",
    "    return pos_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d4715",
   "metadata": {},
   "source": [
    "Just check that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f9b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015901647125836617"
     ]
    }
   ],
   "source": [
    "fraud_logreg_train(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62e37b",
   "metadata": {},
   "source": [
    "Now we have to define the search space, meaning the interval to be searched for the best value. We are trying to optimize the positive-class precision, and since the positive class is very rare, the weight on it should probably be large. Let's try between 10 and 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1372638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter added: class_weight"
     ]
    }
   ],
   "source": [
    "from maggy import Searchspace\n",
    "\n",
    "sp = Searchspace(class_weight=('DOUBLE', [10, 10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1e8d0",
   "metadata": {},
   "source": [
    "Now we can run the `lagom` method, which tries to find the best value. Lagom is a Swedish word that means \"just right\". We give it the training wrapper function and the search space we just defined. Since precision is a metric we want to be as high as possible, we use `direction=max`. \n",
    "\n",
    "`num_trials` is simply how many models will be trained; you should set this based on how much time you are willing to spend. We'll just do five trials here to showcase the functionality.\n",
    "\n",
    "The `optimizer` is the strategy used to determine the next parameter value to try. We will just ise random search, which often works well in practice. You can read about alternatives [here](https://maggy.ai/master/hpo/strategies/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573d5b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Maggy Experiment: fraud_lr, application_1649849762999_0193, run 1\n",
      "\n",
      "------ RandomSearch Results ------ direction(max) \n",
      "BEST combination {\"class_weight\": 2055.11695843451} -- metric 0.006215254998100894\n",
      "WORST combination {\"class_weight\": 5460.944535964473} -- metric 0.0020668741922343866\n",
      "AVERAGE metric -- 0.0030176014605893644\n",
      "EARLY STOPPED Trials -- 0\n",
      "Total job time 0 hours, 0 minutes, 46 seconds\n",
      "\n",
      "Finished ExperimentWARN: Can't reach Maggy server. No progress information and logs available. Job continues running anyway.\n"
     ]
    }
   ],
   "source": [
    "from maggy import experiment\n",
    "\n",
    "result = experiment.lagom(train_fn=fraud_logreg_train,\n",
    "                            searchspace=sp,\n",
    "                            optimizer='randomsearch',\n",
    "                            direction='max',\n",
    "                            num_trials=5,\n",
    "                            name='fraud_lr'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e2c3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_id': '908fd86e654a748f', 'best_val': 0.006215254998100894, 'best_hp': {'class_weight': 2055.11695843451}, 'worst_id': '17dabed92656e58c', 'worst_val': 0.0020668741922343866, 'worst_hp': {'class_weight': 5460.944535964473}, 'avg': 0.0030176014605893644, 'metric_list': [0.0020785289476878263, 0.0026303675938712436, 0.0020668741922343866, 0.006215254998100894, 0.0020969815710524704], 'num_trials': 5, 'early_stopped': 0}"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4184c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
