{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df6d88d",
   "metadata": {},
   "source": [
    "## Model serving with HSML\n",
    "\n",
    "In this example, we are going to serve the model that we created in the model training notebook.\n",
    "\n",
    "For the example to work, you need to have serving enabled in your project. In the settings tab for your project, select Serving to enable it. Now your UI should show a new tab called Model Serving.\n",
    "\n",
    "A model deployment (also called \"model serving\") can be created directly in the Hopsworks UI, by clicking on Model Serving and then on Create New Serving. In this example, however, we will create it through code with the HSML library.\n",
    "\n",
    "![tutorial-flow](images/end_to_end.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2b958",
   "metadata": {},
   "source": [
    "### About Model Serving\n",
    "\n",
    "Models can be served via KFServing or \"default\" serving, which means a Docker container exposing a Flask server. For KFServing models, or models written in Tensorflow, you do not need to write a prediction file (see the section below). However, for sklearn models using default serving, you do need to proceed to write a prediction file.\n",
    "\n",
    "In order to use KFServing, you must have Kubernetes installed and enabled on your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f98ff",
   "metadata": {},
   "source": [
    "### Create the Prediction File\n",
    "\n",
    "In order to deploy a model, you need to write a Python file containing the logic to return a prediction from the model. Don't worry, this is usually a matter of just modifying some paths in a template script. An example can be seen in the code block below, where we have taken [this](https://hopsworks.readthedocs.io/en/latest/hopsml/python_model_serving.html#serving-python-based-models-on-hopsworks) Scikit-learn template script and changed two paths (see comments). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744ee1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predict_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict_example.py\n",
    "from sklearn.externals import joblib\n",
    "from hops import hdfs # TODO this library should not be needed.\n",
    "\n",
    "class Predict(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the serving state, reads a trained model from HDFS\"\"\"\n",
    "        self.model_path = \"Models/fraud_tutorial_model/1/model.pkl\" # Changed to our path.\n",
    "        print(\"Copying Scikit-Learn model from HDFS to local directory\")\n",
    "        hdfs.copy_to_local(self.model_path)\n",
    "        print(\"Reading local Scikit-Learn model for serving\")\n",
    "        self.model = joblib.load(\"./model.pkl\") # Changed to our path.\n",
    "        print(\"Initialization Complete\")\n",
    "\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Serves a prediction request usign a trained model\"\"\"\n",
    "        return self.model.predict(inputs).tolist() # Numpy Arrays are not JSON serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87790f97",
   "metadata": {},
   "source": [
    "If you wonder why we use the path `Models/fraud_tutorial_model/1/model.pkl`, it is useful to know that the Data Sets tab in the Hopsworks UI lets you browse among the different files in the project. Registered models will be found underneath the `Models` directory. Since we saved our model with the name `fraud_tutorial_model`, that's the directory we should look in. `1` is just the version of the model we want to deploy.\n",
    "\n",
    "This script needs to be put into a known location in the Hopsworks file system. Let's call the file `predict_example.py` and put it in the `Models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e378809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5815ca119f8347e79a34775bb0957603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Uploading', max=800.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hopsworks\n",
    "\n",
    "hopsworks_conn = hopsworks.connection()\n",
    "project = hopsworks_conn.get_project()\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "uploaded_file_path = dataset_api.upload(\"predict_example.py\", \"Models\")\n",
    "predictor_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99850e9e",
   "metadata": {},
   "source": [
    "## Create the deployment\n",
    "\n",
    "Here, we fetch the model we want from the model registry and define a configuration for the deployment. For the configuration, we need to specify the serving type (default or KFserving) and in this case, since we use default serving and an sklearn model, we need to give the location of the prediction script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645e6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()\n",
    "\n",
    "# Use the model name from the previous notebook.\n",
    "model = mr.get_model(\"fraud_tutorial_model\", version=1)\n",
    "\n",
    "# Give it any name you want\n",
    "deployment = model.deploy(\n",
    "    name=\"frauddeployment\", \n",
    "    model_server=\"PYTHON\",\n",
    "    serving_tool=\"DEFAULT\",\n",
    "    script_file=predictor_script_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705d05d",
   "metadata": {},
   "source": [
    "The deployment has now been registered. However, to start it you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4da3a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3b2964f73f4bd88254cbe0f3ac0fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deployment.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2278c",
   "metadata": {},
   "source": [
    "### Using the deployment\n",
    "\n",
    "Let's use the input example that we registered together with the model to query the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171be49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0237707765302748, 0.6177866631017085, 0.4141638755976203, 0.0875667692271739, 0.0237707765302748, 0.0, 0.0, 0.0890611654588838, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://hopsworks.glassfish.service.consul:8182/hopsworks-api/api/project/119/inference/models/frauddeployment:predict). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, error code: 250002, error msg: Error contacting the serving server, user msg: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8a36dc60f194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdeployment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsml/deployment.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serving_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsml/engine/serving_engine.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, deployment_instance, data)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mserving_tool\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPREDICTOR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSERVING_TOOL_KSERVE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         )  # if not KServe, send request to Hopsworks\n\u001b[0;32m--> 117\u001b[0;31m         return self._serving_api.send_inference_request(\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mdeployment_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthrough_hopsworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsml/core/serving_api.py\u001b[0m in \u001b[0;36msend_inference_request\u001b[0;34m(self, deployment_instance, data, through_hopsworks)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_url\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         return _client._send_request(\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsml/decorators.py\u001b[0m in \u001b[0;36mif_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNoHopsworksConnectionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mif_connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsml/client/base.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRestAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://hopsworks.glassfish.service.consul:8182/hopsworks-api/api/project/119/inference/models/frauddeployment:predict). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, error code: 250002, error msg: Error contacting the serving server, user msg: "
     ]
    }
   ],
   "source": [
    "test_inputs = [model.input_example]\n",
    "print(test_inputs)\n",
    "\n",
    "data = {\n",
    "    \"inputs\": test_inputs\n",
    "}\n",
    "\n",
    "deployment.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a4c9c",
   "metadata": {},
   "source": [
    "### Use REST endpoint\n",
    "\n",
    "You can also use a REST endpoint for your model. To do this you need to create an API key with 'serving' enabled, and retrieve the endpoint URL from the Model Serving UI.\n",
    "\n",
    "(**NOTE currently this has to be done in the old UI**)\n",
    "\n",
    "Go to the Model Serving UI and click on the eye icon next to a model to retrieve the endpoint URL. The shorter URL is an internal endpoint that you can only reach from within Hopsworks. If you want to call it from outside, you need one of the longer URLs. Make sure to use https instead of http. (**TODO this should be fixed**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84aeb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"\"  # Put your API key here.\n",
    "\n",
    "data = {\"inputs\": test_inputs}\n",
    "url = \"\"  # Put the URL you found in the UI here.\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\", \"Accept\": \"application/json\",\n",
    "    \"Authorization\": f\"ApiKey {API_KEY}\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fcd452",
   "metadata": {},
   "source": [
    "### Stop Deployment\n",
    "\n",
    "To stop the deployment we simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f65b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07af2a",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook we'll take a look at how to automate jobs in Hopsworks using Airflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}