{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96bcca12",
   "metadata": {},
   "source": [
    "## Model serving with HSML\n",
    "\n",
    "In this example, we are going to serve the model that we created in the model training notebook.\n",
    "\n",
    "For the example to work, you need to have serving enabled in your project. In the settings tab for your project, select Serving to enable it. Now your UI should show a new tab called Model Serving.\n",
    "\n",
    "A model deployment (also called \"model serving\") can be created directly in the Hopsworks UI, by clicking on Model Serving and then on Create New Serving. In this example, however, we will create it through code with the HSML library.\n",
    "\n",
    "![tutorial-flow](images/end_to_end.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45b211",
   "metadata": {},
   "source": [
    "### About Model Serving\n",
    "\n",
    "Models can be served via KFServing or \"default\" serving, which means a Docker container exposing a Flask server. For KFServing models, or models written in Tensorflow, you do not need to write a prediction file (see the section below). However, for sklearn models using default serving, you do need to proceed to write a prediction file.\n",
    "\n",
    "In order to use KFServing, you must have Kubernetes installed and enabled on your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d539c",
   "metadata": {},
   "source": [
    "### Create the Prediction File\n",
    "\n",
    "In order to deploy a model, you need to write a Python file containing the logic to return a prediction from the model. Don't worry, this is usually a matter of just modifying some paths in a template script. An example can be seen in the code block below, where we have taken [this](https://hopsworks.readthedocs.io/en/latest/hopsml/python_model_serving.html#serving-python-based-models-on-hopsworks) Scikit-learn template script and changed two paths (see comments). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29381b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from hops import hdfs # TODO this library should not be needed.\n",
    "\n",
    "class Predict(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the serving state, reads a trained model from HDFS\"\"\"\n",
    "        self.model_path = \"Models/fraud_tutorial_model/1/model.pkl\" # Changed to our path.\n",
    "        print(\"Copying Scikit-Learn model from HDFS to local directory\")\n",
    "        hdfs.copy_to_local(self.model_path)\n",
    "        print(\"Reading local Scikit-Learn model for serving\")\n",
    "        self.model = joblib.load(\"./model.pkl\") # Changed to our path.\n",
    "        print(\"Initialization Complete\")\n",
    "\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Serves a prediction request usign a trained model\"\"\"\n",
    "        return self.model.predict(inputs).tolist() # Numpy Arrays are not JSON serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5525f",
   "metadata": {},
   "source": [
    "If you wonder why we use the path `Models/fraud_tutorial_model/1/model.pkl`, it is useful to know that the Data Sets tab in the Hopsworks UI lets you browse among the different files in the project. Registered models will be found underneath the `Models` directory. Since we saved our model with the name `fraud_tutorial`, that's the directory we should look in. `1` is just the version of the model we want to deploy.\n",
    "\n",
    "This script needs to be put into a known location in the Hopsworks file system. Let's call the file `predict_example.py` and put it in the `Models` directory. The following should be the path to the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_SCRIPT_PATH = \"/Projects/fraud_tutorial/Models/predict_example.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78bbaf",
   "metadata": {},
   "source": [
    "## Create the deployment\n",
    "\n",
    "Here, we fetch the model we want from the model registry and define a configuration for the deployment. For the configuration, we need to specify the serving type (default or KFserving) and in this case, since we use default serving and an sklearn model, we need to give the location of the prediction script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsml\n",
    "from hsml.predictor_config import PredictorConfig\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()\n",
    "\n",
    "# Use the model name from the previous notebook.\n",
    "model = mr.get_model(\"fraud_tutorial_model\", version=1)\n",
    "\n",
    "predictor_config = PredictorConfig(\n",
    "    model_server=\"PYTHON\",\n",
    "    serving_tool=\"DEFAULT\",\n",
    "    script_file=PREDICTOR_SCRIPT_PATH,\n",
    ")\n",
    "\n",
    "# Give it any name you want\n",
    "deployment = model.deploy(name=\"frauddeployment\", predictor_config=predictor_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7dd71",
   "metadata": {},
   "source": [
    "The deployment has now been registered. However, to start it you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfda883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO These two lines should not be necessary (?) once the ID error has been resolved.\n",
    "ms = conn.get_model_serving()\n",
    "deployment = ms.get_deployment(\"frauddeployment\")\n",
    "\n",
    "deployment.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900f28a",
   "metadata": {},
   "source": [
    "### Using the deployment\n",
    "\n",
    "Let's use the input example that we registered together with the model to query the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3287558",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [model.input_example]\n",
    "print(test_inputs)\n",
    "\n",
    "data = {\n",
    "    \"inputs\": test_inputs\n",
    "}\n",
    "\n",
    "deployment.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124d985",
   "metadata": {},
   "source": [
    "### Use REST endpoint\n",
    "\n",
    "You can also use a REST endpoint for your model. To do this you need to create an API key with 'serving' enabled, and retrieve the endpoint URL from the Model Serving UI.\n",
    "\n",
    "(**NOTE currently this has to be done in the old UI**)\n",
    "\n",
    "Go to the Model Serving UI and click on the eye icon next to a model to retrieve the endpoint URL. The shorter URL is an internal endpoint that you can only reach from within Hopsworks. If you want to call it from outside, you need one of the longer URLs. Make sure to use https instead of http. (**TODO this should be fixed**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f63318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"\"  # Put your API key here.\n",
    "\n",
    "data = {\"inputs\": test_inputs}\n",
    "url = \"\"  # Put the URL you found in the UI here.\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\", \"Accept\": \"application/json\",\n",
    "    \"Authorization\": f\"ApiKey {API_KEY}\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f182367",
   "metadata": {},
   "source": [
    "### Stop Deployment\n",
    "\n",
    "To stop the deployment we simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f22b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2bd005",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a92d9a",
   "metadata": {},
   "source": [
    "## (Work in progress) Simple GUI with Gradio\n",
    "\n",
    "In order to have users input natural features (age in years, transaction amounts in euros etc.), we would need to have the normalization constants available here (i.e. the min/max values that were used for min-max scaling). Then the normalized feature values could be computed. \n",
    "\n",
    "For now, we will have to make do with an interface where users input min-max scaled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9934dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871/\n",
      "2022-04-25 12:18:26,912 INFO: Connected (version 2.0, client OpenSSH_7.6p1)\n",
      "2022-04-25 12:18:27,737 INFO: Authentication (publickey) successful!\n",
      "Running on public URL: https://58788.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://58788.gradio.app\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f85bc6bbf70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def get_prediction(cat, tx_vol_mean, tx_vol_std, tx_amt, tx_frq, age, days_exp, loc_delta, loc_delta_avg):\n",
    "    '''\n",
    "    The Gradio interface does an implicit mapping between category values and ints, so we don't need to \n",
    "    specify that mapping explicitly here.\n",
    "    '''\n",
    "    feat_vec = [[cat, tx_vol_mean, tx_vol_std, tx_amt, tx_frq, age, days_exp, loc_delta, loc_delta_avg]]\n",
    "    res = deployment.predict(data)\n",
    "    prediction = res['predictions'][0]\n",
    "    return prediction\n",
    "\n",
    "\n",
    "input_ui = [\n",
    "        gr.inputs.Dropdown(list(category_mapping.keys()), type=\"index\", label=\"category\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"4h avg transaction volume (normalized)\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"4h avg transaction volume (normalized)\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"Transaction amount (normalized\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"Transaction frequency (normalized\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"Age at transaction (normalized)\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"Days until card expires (normalized)\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"Location delta (normalized)\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"4h avg location delta (normalized)\"),\n",
    "    ]\n",
    "\n",
    "iface = gr.Interface(fn=get_prediction, inputs=input_ui, outputs=\"label\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290f5e3",
   "metadata": {},
   "source": [
    "Now you can change the values with the slider and press Submit at the bottom. A prediction will then appear in the box labeled \"Output\" in the upper right column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
