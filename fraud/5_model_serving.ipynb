{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96bcca12",
   "metadata": {},
   "source": [
    "## Model serving with HSML\n",
    "\n",
    "In this example, we are going to serve the model that we created in the model training notebook.\n",
    "\n",
    "For the example to work, you need to have serving enabled in your project. In the settings tab for your project, select Serving to enable it. Now your UI should show a new tab called Model Serving.\n",
    "\n",
    "A model deployment (also called \"model serving\") can be created directly in the Hopsworks UI, by clicking on Model Serving and then on Create New Serving. In this example, however, we will create it through code with the HSML library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45b211",
   "metadata": {},
   "source": [
    "## About model serving\n",
    "\n",
    "Models can be served via KFServing or \"default\" serving, which means a Docker container exposing a Flask server. For KFServing models, or models written in Tensorflow, you do not need to write a prediction file (see the section below). However, for sklearn models using default serving, you do need to proceed to write a prediction file.\n",
    "\n",
    "In order to use KFServing, you must have Kubernetes installed and enabled on your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d539c",
   "metadata": {},
   "source": [
    "### Create the prediction file (only necessary for sklearn models with default serving)\n",
    "\n",
    "In order to deploy a model, you need to write a Python file containing the logic to return a prediction from the model. Don't worry, this is usually a matter of just modifying some paths in a template script.\n",
    "\n",
    "There is an example of such a template script [here](https://hopsworks.readthedocs.io/en/latest/hopsml/python_model_serving.html). For our case, we can modify it so that the `__init__` method reads as follows (the rest can be left as is):\n",
    "\n",
    "```\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the serving state, reads a trained model from HDFS\"\"\"\n",
    "        self.model_path = \"Models/fraud_tutorial_model/1/model.pkl\"\n",
    "        print(\"Copying Scikit-Learn model from HDFS to local directory\")\n",
    "        hdfs.copy_to_local(self.model_path)\n",
    "        print(\"Reading local Scikit-Learn model for serving\")\n",
    "        self.model = joblib.load(\"./model.pkl\")\n",
    "        print(\"Initialization Complete\")\n",
    "```\n",
    "\n",
    "If you wonder why we use the path `Models/fraud_tutorial_model/2/model.pkl`, it is useful to know that the Data Sets tab in the Hopsworks UI lets you browse among the different files in the project. Registered models will be found underneath the `Models` directory. Since we saved our model with the name `fraud_tutorial`, that's the directory we shpuld look in. `1` is just the version of the model we want to deploy.\n",
    "\n",
    "This script needs to be put into a known location in the Hopsworks file system. Let's call the file `predict_example.py` and put it in the `Models` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78bbaf",
   "metadata": {},
   "source": [
    "## Create the deployment\n",
    "\n",
    "Here, we fetch the model we want from the model registry and define a configuration for the deployment. For the configuration, we need to specify the serving type (default or KFserving) and in this case, since we use default serving and an sklearn model, we need to give the location of the prediction script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsml\n",
    "from hsml.predictor_config import PredictorConfig\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()\n",
    "\n",
    "# Use the location where you saved the prediction file\n",
    "PREDICTOR_SCRIPT = \"/Projects/fraud_tutorial/Models/predict_example.py\"\n",
    "\n",
    "# Use the model name from notebook 4, where we registered the model\n",
    "model = mr.get_model('fraud_tutorial_model', version=1)\n",
    "\n",
    "predictor_config = PredictorConfig(model_server=\"PYTHON\",\n",
    "                                   serving_tool=\"DEFAULT\",\n",
    "                                   script_file=PREDICTOR_SCRIPT,\n",
    "                                    )\n",
    "\n",
    "# Give it any name you want\n",
    "model.deploy(name='frauddeployment3', predictor_config=predictor_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7dd71",
   "metadata": {},
   "source": [
    "Your new deployment should now be visible in the UI under Model Serving. Press run to start the deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900f28a",
   "metadata": {},
   "source": [
    "### Using the deployment\n",
    "\n",
    "Let's create a fake data point with which we can query the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3287558",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [[4.00000000e+00, 2.87415201e-06, 2.87415201e-06,\n",
    "        0.00000000e+00, 4.79025335e-05, 1.26263643e-05, 1.26263643e-05,\n",
    "        1.97967580e-03, 2.79541810e-02],\n",
    "       [4.00000000e+00, 3.02887719e-03, 3.01897734e-03,\n",
    "        3.78048219e-04, 1.43707600e-04, 3.89754901e-06, 5.64310547e-06,\n",
    "        4.37811982e-03, 5.83522294e-02]]\n",
    "\n",
    "\n",
    "data = {\n",
    "            \"inputs\": test_inputs\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3200443f",
   "metadata": {},
   "source": [
    "### Fetch the serving via HSML\n",
    "\n",
    "... and make the prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de51f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = conn.get_model_serving()\n",
    "deployment = ms.get_deployment('frauddeployment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbe7f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124d985",
   "metadata": {},
   "source": [
    "### Use REST endpoint\n",
    "\n",
    "We can also use the REST endpoint that you can find in the Model Serving UI by clicking on the eye icon next to a model. The shorter URL is an internal endpoint that you can only reach from within Hopsworks. If you want to call it from outside, you need one of the longer URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f63318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data = {\"inputs\": test_inputs}\n",
    "url = 'https://791bb4a0-bb1c-11ec-8721-7bd8cdac0b54.cloud.hopsworks.ai:443/hopsworks-api/api/project/120/inference/models/frauddeployment3:predict' # Found in the UI\n",
    "headers = {'Content-Type': 'application/json', 'Accept':'application/json'}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3778308a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'restApiJsonResponse',\n",
       " 'errorCode': 200003,\n",
       " 'errorMsg': 'Authorization header not set.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a92d9a",
   "metadata": {},
   "source": [
    "## (Work in progress) Simple GUI with Gradio\n",
    "\n",
    "In order to have users input natural features (age in years, transaction amounts in euros etc.), we would need to have the normalization constants available here (i.e. the min/max values that were used for min-max scaling). Then the normalized feature values could be computed. \n",
    "\n",
    "For now, we will have to make do with an interface where users input min-max scaled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9934dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871/\n",
      "2022-04-25 12:18:26,912 INFO: Connected (version 2.0, client OpenSSH_7.6p1)\n",
      "2022-04-25 12:18:27,737 INFO: Authentication (publickey) successful!\n",
      "Running on public URL: https://58788.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://58788.gradio.app\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f85bc6bbf70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def get_prediction(cat, tx_vol_mean, tx_vol_std, tx_amt, tx_frq, age, days_exp, loc_delta, loc_delta_avg):\n",
    "    '''\n",
    "    The Gradio interface does an implicit mapping between category values and ints, so we don't need to \n",
    "    specify that mapping explicitly here.\n",
    "    '''\n",
    "    feat_vec = [[cat, tx_vol_mean, tx_vol_std, tx_amt, tx_frq, age, days_exp, loc_delta, loc_delta_avg]]\n",
    "    res = deployment.predict(data)\n",
    "    prediction = res['predictions'][0]\n",
    "    return prediction\n",
    "\n",
    "\n",
    "input_ui = [\n",
    "        gr.inputs.Dropdown(list(category_mapping.keys()), type=\"index\", label=\"category\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"4h avg transaction volume (normalized)\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"4h avg transaction volume (normalized)\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"Transaction amount (normalized\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"Transaction frequency (normalized\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"Age at transaction (normalized)\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"Days until card expires (normalized)\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"Location delta (normalized)\"),\n",
    "        gr.inputs.Slider(0, 1, label=\"4h avg location delta (normalized)\"),\n",
    "    ]\n",
    "\n",
    "iface = gr.Interface(fn=get_prediction, inputs=input_ui, outputs=\"label\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290f5e3",
   "metadata": {},
   "source": [
    "Now you can change the values with the slider and press Submit at the bottom. A prediction will then appear in the box labeled \"Output\" in the upper right column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
