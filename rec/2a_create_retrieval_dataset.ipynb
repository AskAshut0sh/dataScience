{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Retrieval Dataset\n",
    "\n",
    "In this notebook, we'll create a dataset for our retrieval model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "First, we'll load the feature groups we created in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fg = fs.get_feature_group(\"transactions\")\n",
    "customers_fg = fs.get_feature_group(\"customers\")\n",
    "articles_fg = fs.get_feature_group(\"articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to join these three data sources to make the data compatible with out retrieval model. Recall that each row in the `transactions` feature group relates information about which customer bought which item. We'll join this feature group with the `customers` and `articles` feature groups to inject customer and item features into each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = trans_fg.select([\"customer_id\", \"article_id\", \"month_sin\", \"month_cos\"])\\\n",
    "    .join(customers_fg.select([\"age\"]), on=\"customer_id\")\\\n",
    "    .join(articles_fg.select([\"garment_group_name\", \"index_group_name\"]), on=\"article_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Functions\n",
    "Transformation functions are a mathematical mapping of input data that may be stateful - requiring statistics from the parent feature view (such as number of instances of a category, or mean value of a numerical feature)\n",
    "\n",
    "We will preprocess our data using min-max scaling on numerical features and label encoding on categorical features. To do this we simply define a mapping between our features and transformation functions. This ensures that transformation functions such as min-max scaling are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformation functions.\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
    "\n",
    "# Map features to transformations.\n",
    "transformation_functions = {\n",
    "    \"age\" : min_max_scaler,\n",
    "    # \"garment_group_name\" : label_encoder,\n",
    "    # \"index_group_name\" : label_encoder\n",
    "}\n",
    "\n",
    "# TODO for some reason label_encoder seems to cause problems when doing td.read()?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature View Creation\n",
    "In Hopsworks, you write features to feature groups (where the features are stored) and you read features from feature views. A feature view is a logical view over features, stored in feature groups, and a feature view typically contains the features used by a specific model. This way, feature views enable features, stored in different feature groups, to be reused across many different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.create_feature_view(\n",
    "    name='retrieval_fv',\n",
    "    query=query,\n",
    "    transformation_functions=transformation_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view and explore data in the feature view we can retrieve batch data using the `get_batch_data()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view.get_batch_data().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset Creation\n",
    "\n",
    "Finally, we can create our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO we will use a chronological split instead.\n",
    "\n",
    "td = feature_view.create_training_dataset(\n",
    "    description = 'retrieval_dataset_splitted',\n",
    "    data_format = 'csv',\n",
    "    splits = {'train': 80, 'validation': 20},\n",
    "    train_split = \"train\",\n",
    "    write_options = {'wait_for_job': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll train a model on the dataset we created in this notebook."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
