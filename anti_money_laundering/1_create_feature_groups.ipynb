{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08b14ba",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 01: Load, Engineer & Connect</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\"> This is the first part of the quick start series of tutorials about Hopsworks Feature Store. As part of this first module, we will work with data related to credit card transactions. \n",
    "The objective of this tutorial is to demonstrate how to work with the **Hopworks Feature Store**  for batch data with a goal of training and deploying a model that can predict fraudulent transactions.</span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided in 4 sections:** \n",
    "1. Loading the data and do feature engineeing,\n",
    "2. Connect to the Hopsworks feature store,\n",
    "3. Create feature groups and upload them to the feature store.\n",
    "4. Explore feature groups from the UI.\n",
    "\n",
    "![tutorial-flow](../images/01_featuregroups.png)\n",
    "\n",
    "First of all we will load the data and do some feature engineering on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c50fc",
   "metadata": {},
   "source": [
    "### üìù Import librararies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5514e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for feature engineering\n",
    "# common libaries for hashing and date time conversions\n",
    "import hashlib\n",
    "import datetime\n",
    "\n",
    "# pandas for feature engineering \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ffcdc6",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üíΩ Loading the Data </span>\n",
    "\n",
    "The data we will use comes from three different CSV files:\n",
    "\n",
    "- `transactions.csv`: transaction information such as timestamp, location, and the amount. \n",
    "- `alert_transactions.csv`: Suspicious Activity Report (SAR) transactions.\n",
    "- `party.csv`: User profile information.\n",
    "\n",
    "In a production system, these CSV files would originate from separate data sources or tables, and probably separate data pipelines. **All three files have a customer id column `id` in common, which we can use for joins.**\n",
    "\n",
    "Let's go ahead and load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2489d3",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Transactions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4141e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/transactions.csv\", parse_dates = ['tran_timestamp'])\n",
    "transactions_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed660e2b",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Alert Transactions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea05e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/alert_transactions.csv\")\n",
    "alert_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7a982",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Party dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "party = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/party.csv\")\n",
    "party.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabc651",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üõ†Ô∏è Feature Engineering </span>\n",
    "\n",
    "#### To investigate patterns of suspicious activities you will make time window aggregates such monthly frequency, total, mean and standard deviation of amount of incoming and outgoing transasactions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.columns = ['tran_id', 'tx_type', 'base_amt', 'tran_timestamp', 'source', 'target']\n",
    "transactions_df = transactions_df[[\"source\",\"target\",\"tran_timestamp\",\"tran_id\", \"base_amt\"]]\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6b98c6",
   "metadata": {},
   "source": [
    "##### Outgoing transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = transactions_df.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'source'])\\\n",
    "                            .agg(monthly_count=('source','count'), \n",
    "                                 monthly_total_amount=('base_amt','sum'),\n",
    "                                 monthly_mean_amount=('base_amt','mean'),\n",
    "                                 monthly_std_amount=('base_amt','std')\n",
    "                                )\n",
    "out_df = out_df.reset_index(level=[\"source\"])\n",
    "out_df = out_df.reset_index(level=[\"tran_timestamp\"])\n",
    "out_df.columns  = [\"tran_timestamp\", \"id\", \"monthly_out_count\", \"monthly_out_total_amount\", \"monthly_out_mean_amount\", \"monthly_out_std_amount\"]\n",
    "out_df.tran_timestamp = out_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "out_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b1a8de",
   "metadata": {},
   "source": [
    "##### Incoming transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = transactions_df.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'target'])\\\n",
    "                            .agg(monthly_count=('target','count'), \n",
    "                                 monthly_total_amount=('base_amt','sum'),\n",
    "                                 monthly_mean_amount=('base_amt','mean'),\n",
    "                                 monthly_std_amount=('base_amt','std'))\n",
    "\n",
    "in_df = in_df.reset_index(level=[\"target\"])\n",
    "in_df = in_df.reset_index(level=[\"tran_timestamp\"])\n",
    "in_df.columns  = [\"tran_timestamp\", \"id\", \"monthly_in_count\", \"monthly_in_total_amount\", \"monthly_in_mean_amount\", \"monthly_in_std_amount\"]\n",
    "in_df.tran_timestamp = in_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "in_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13151cf",
   "metadata": {},
   "source": [
    "##### Now lets join incoming and outgoing transcations datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74217d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_df = in_df.merge(out_df, on=['tran_timestamp', 'id'], how=\"outer\")\n",
    "in_out_df =  in_out_df.fillna(0)\n",
    "in_out_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100aabe0",
   "metadata": {},
   "source": [
    "#### Assign labels to transuctons that were identified as suspicius activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_labels = transactions_df[[\"source\",\"target\",\"tran_id\",\"tran_timestamp\"]].merge(alert_transactions[[\"is_sar\", \"tran_id\"]], on=[\"tran_id\"], how=\"left\")\n",
    "transaction_labels.is_sar = transaction_labels.is_sar.map({True: 1, np.nan: 0})\n",
    "transaction_labels.sort_values('tran_id',inplace = True)\n",
    "transaction_labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ccc046",
   "metadata": {},
   "source": [
    "#### Now lets prepare profile (party) dataset and assign lables whether they have been reported for suspicius activity or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5702e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "party.columns = [\"id\",\"type\"]\n",
    "party.type = party.type.map({\"Individual\": 0, \"Organization\": 1})\n",
    "\n",
    "party.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = transaction_labels[transaction_labels.is_sar ==1]\n",
    "alert_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c81b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = transaction_labels[transaction_labels.is_sar ==1]\n",
    "alert_sources = alert_transactions[[\"source\", \"tran_timestamp\"]]\n",
    "alert_sources.columns = [\"id\", \"tran_timestamp\"]\n",
    "alert_sources.head()\n",
    "alert_targets = alert_transactions[[\"target\", \"tran_timestamp\"]]\n",
    "alert_targets.columns = [\"id\", \"tran_timestamp\"]\n",
    "sar_party = alert_sources.append(alert_targets, ignore_index=True)\n",
    "sar_party.sort_values([\"id\", \"tran_timestamp\"], ascending = [False, True])\n",
    "\n",
    "# find a 1st occurence of sar per id\n",
    "sar_party = sar_party.iloc[[sar_party.id.eq(id).idxmax() for id in sar_party['id'].value_counts().index]]\n",
    "sar_party = sar_party.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'id']).agg(monthly_count=('id','count'))\n",
    "sar_party = sar_party.reset_index(level=[\"id\"])\n",
    "sar_party = sar_party.reset_index(level=[\"tran_timestamp\"])\n",
    "sar_party.drop([\"monthly_count\"], axis=1, inplace=True)\n",
    "\n",
    "sar_party[\"is_sar\"] = sar_party[\"is_sar\"] = 1\n",
    "sar_party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12755ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_labels = party.merge(sar_party, on=[\"id\"], how=\"left\")\n",
    "party_labels.is_sar = party_labels.is_sar.map({1.0: 1, np.nan: 0})\n",
    "max_time_stamp = datetime.datetime.utcfromtimestamp(int(max(transaction_labels.tran_timestamp.values))/1e9)\n",
    "party_labels = party_labels.fillna(max_time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a164897b",
   "metadata": {},
   "source": [
    "### Graph representational learning using graph convolution layer\n",
    "\n",
    "Finanial transactions can be represented as a dynamic network graph. Using technique of graph representation \n",
    "give as opportunity to represnet transaction with a broader context. In this examples we will perfom node \n",
    "representation leaning. \n",
    "\n",
    "Network architecture of the graph convolution layer for learning node represantion learning  was taken from \n",
    "[this Keras example](https://keras.io/examples/graph/gnn_citations/).  It performs the following steps:\n",
    "\n",
    "1. **Prepare**: The input node representations are processed using a FFN to produce a *message*. You can simplify\n",
    "the processing by only applying linear transformation to the representations.\n",
    "2. **Aggregate**: The messages of the neighbours of each node are aggregated with\n",
    "respect to the `edge_weights` using a *permutation invariant* pooling operation, such as *sum*, *mean*, and *max*,\n",
    "to prepare a single aggregated message for each node. See, for example, [tf.math.unsorted_segment_sum](https://www.tensorflow.org/api_docs/python/tf/math/unsorted_segment_sum)\n",
    "APIs used to aggregate neighbour messages.\n",
    "3. **Update**: The `node_repesentations` and `aggregated_messages`‚Äîboth of shape `[num_nodes, representation_dim]`‚Äî\n",
    "are combined and processed to produce the new state of the node representations (node embeddings).\n",
    "If `combination_type` is `gru`, the `node_repesentations` and `aggregated_messages` are stacked to create a sequence,\n",
    "then processed by a GRU layer. Otherwise, the `node_repesentations` and `aggregated_messages` are added\n",
    "or concatenated, then processed using a FFN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to compute graph embeddings \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)\n",
    "\n",
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.2,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        if self.combination_type == \"gated\":\n",
    "            self.update_fn = layers.GRU(\n",
    "                units=hidden_units,\n",
    "                activation=\"tanh\",\n",
    "                recurrent_activation=\"sigmoid\",\n",
    "                dropout=dropout_rate,\n",
    "                return_state=True,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "            )\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        # node_repesentations shape is [num_edges, embedding_dim].\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages):\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        num_nodes = tf.math.reduce_max(node_indices) + 1\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        if self.combination_type == \"gru\":\n",
    "            # Create a sequence of two elements for the GRU layer.\n",
    "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            # Concatenate the node_repesentations and aggregated_messages.\n",
    "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            # Add node_repesentations and aggregated_messages.\n",
    "            h = node_repesentations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        # Apply the processing function.\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "\n",
    "        # Prepare the messages of the neighbours.\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_messages = self.aggregate(node_indices, neighbour_messages)\n",
    "        # Update the node embedding with the neighbour messages.\n",
    "        return self.update(node_repesentations, aggregated_messages)\n",
    "\n",
    "\n",
    "class GNNNodeClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GNNNodeClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = layers.Dense(hidden_units[0],  activation=tf.nn.tanh, name=\"logits\")\n",
    "        \n",
    "\n",
    "    def call(self, input_node_indices):\n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(self.node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.gather(x, input_node_indices)\n",
    "        # Compute logits\n",
    "        return self.compute_logits(node_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_gruph(input_df):\n",
    "    sampled_party = party_labels[party_labels.id.isin(input_df.source) | (party_labels.id.isin(input_df.target))]\n",
    "    sampled_party = sampled_party [[\"id\", \"type\", \"is_sar\"]]\n",
    "\n",
    "    # assigne unquie interger ids to each node to be compatible with thensorlfow\n",
    "    unique_ids = set()\n",
    "    for id in sampled_party.id.values:\n",
    "      unique_ids.add(id)\n",
    "    id_dict = {}\n",
    "\n",
    "    for i, idn in enumerate(unique_ids):\n",
    "        id_dict[idn]=i\n",
    "\n",
    "    sampled_party['int_id'] = sampled_party['id'].apply(lambda x : id_dict[x])\n",
    "    input_df['source'] = input_df['source'].apply(lambda x : id_dict[x])\n",
    "    input_df['target'] = input_df['target'].apply(lambda x : id_dict[x])\n",
    "\n",
    "    # construct graph info\n",
    "    feature_names = [\"type\"]\n",
    "    x_train = sampled_party.int_id.to_numpy()\n",
    "\n",
    "    # Create an edges array (sparse adjacency matrix) of shape [2, num_edges].\n",
    "    edges = input_df[[\"source\", \"target\"]].to_numpy().T\n",
    "\n",
    "    # Create an edge weights array of ones.\n",
    "    edge_weights = tf.ones(shape=edges.shape[1])\n",
    "    # Create a node features array of shape [num_nodes, num_features].\n",
    "    node_features = tf.cast(\n",
    "        sampled_party.sort_values(\"id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
    "    )\n",
    "    # Create graph info tuple with node_features, edges, and edge_weights.\n",
    "    graph_info = (node_features, edges, edge_weights)\n",
    "    \n",
    "    node_features, edges, edge_weights = graph_info\n",
    "\n",
    "    # hyper parameter for graph embeddings model\n",
    "    hidden_units = [32, 32]\n",
    "    learning_rate = 0.01\n",
    "    dropout_rate = 0.5\n",
    "    num_epochs = 2\n",
    "    batch_size = 256\n",
    "    \n",
    "    # Construct the model\n",
    "    model = GNNNodeClassifier(\n",
    "        graph_info=graph_info,\n",
    "        hidden_units=hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        name=\"gnn_model\",\n",
    "    )\n",
    "\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "            #optimizer=keras.optimizers.Adam(learning_rate),\n",
    "            optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "            loss=keras.losses.MeanSquaredError(),    \n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "        )\n",
    "    \n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "            x=x_train,\n",
    "            y=x_train,\n",
    "            epochs=num_epochs,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    graph_embeddings = list(model.predict(x_train).reshape(node_features.shape[0], hidden_units[0]))\n",
    "    # predict and return\n",
    "    return {\"id\": sampled_party.id.to_numpy(), \"graph_embeddings\": graph_embeddings}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6656295d",
   "metadata": {},
   "source": [
    "#### Compute time evolving graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2574797",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_graphs_by_month = transaction_labels.groupby(pd.Grouper(key='tran_timestamp', freq='M')).apply(lambda x: construct_gruph(x))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded49dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = transaction_graphs_by_month.index.values\n",
    "graph_embeddings = transaction_graphs_by_month.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embdeddings_df = pd.DataFrame()\n",
    "for timestamp, graph_embedding in zip(timestamps, graph_embeddings):\n",
    "    df_tmp = pd.DataFrame(graph_embedding)\n",
    "    df_tmp[\"tran_timestamp\"] = timestamp\n",
    "    graph_embdeddings_df = pd.concat([graph_embdeddings_df, df_tmp])    \n",
    "graph_embdeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300d951",
   "metadata": {},
   "source": [
    "#### Convert date time to unix epoc milliseconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_labels.tran_timestamp = transaction_labels.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "graph_embdeddings_df.tran_timestamp = graph_embdeddings_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.map(lambda x: datetime.datetime.timestamp(x) * 1000)\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.values.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea91892",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üëÆüèº‚Äç‚ôÄÔ∏è Data Validation \n",
    "\n",
    "Before you define [feature groups](https://docs.hopsworks.ai/latest/generated/feature_group/) lets define [validation rules](https://docs.hopsworks.ai/latest/generated/feature_validation/) for features. You do expect some of the features to comply with certain *rules* or *expectations*. For example: a transacted amount must be a positive value. In the case of a transacted amount arriving as a negative value you can decide whether to stop it to `write` into a feature group and throw an error or allow it to be written but provide a warning. In the next section you will create feature store `expectations`, attach them to feature groups, and apply them to dataframes being appended to said feature group.\n",
    "\n",
    "#### Data validation with Greate Expectations in Hopsworks\n",
    "You can use GE library for validation in Hopsworks features store. \n",
    "\n",
    "##  <img src=\"../images/icon102.png\" width=\"18px\"></img> Hopsworks feature store\n",
    "\n",
    "The Hopsworks feature feature store library is Apache V2 licensed and available [here](https://github.com/logicalclocks/feature-store-api). The library is currently available for Python and JVM languages such as Scala and Java.\n",
    "In this notebook, we are going to cover Python part.\n",
    "\n",
    "You can find the complete documentation of the library here: \n",
    "\n",
    "The first step is to establish a connection with your Hopsworks feature store instance and retrieve the object that represents the feature store you'll be working with. \n",
    "\n",
    "> By default `connection.get_feature_store()` returns the feature store of the project we are working with. However, it accepts also a project name as parameter to select a different feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184fe07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213bff56",
   "metadata": {},
   "source": [
    "### üî¨ Expectations suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bccdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Expectation Suite - no use of HSFS\n",
    "import great_expectations as ge\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "expectation_suite = ge.core.ExpectationSuite(expectation_suite_name=\"aml_project_validations\")\n",
    "pprint(expectation_suite.to_json_dict(), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_suite.add_expectation(\n",
    "  ge.core.ExpectationConfiguration(\n",
    "  expectation_type=\"expect_column_max_to_be_between\",\n",
    "  kwargs={\"column\": \"monthly_in_count\", \"min_value\": 0, \"max_value\": 10000000}) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b71ef3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Register Feature Groups </span>\n",
    "\n",
    "### Feature Groups\n",
    "\n",
    "A `Feature Groups` is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The `Feature Group` lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them.\n",
    "\n",
    "Generally, the features in a feature group are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, `feature groups` provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in.\n",
    "\n",
    "> It is important to note that `feature groups` are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999eb31d",
   "metadata": {},
   "source": [
    "#### Transactions monthly aggregates feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c867316",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_fg = fs.get_or_create_feature_group(\n",
    "    name = \"transactions_monthly_aml_fg\",\n",
    "    version = 1,\n",
    "    primary_key = [\"id\"],\n",
    "    partition_key = [\"tran_timestamp\"],   \n",
    "    description = \"transactions monthly aggregates features\",\n",
    "    event_time = ['tran_timestamp'],\n",
    "    online_enabled = True,\n",
    "    statistics_config = {\"enabled\": True, \"histograms\": True, \"correlations\": True, \"exact_uniqueness\": False},\n",
    "    expectation_suite=expectation_suite\n",
    ")   \n",
    "\n",
    "transactions_fg.insert(in_out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212f400",
   "metadata": {},
   "source": [
    "#### Alert Transaction labels feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2750ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_labels_fg = fs.get_or_create_feature_group(\n",
    "    name = \"transaction_labels_aml_fg\",\n",
    "    version = 1,\n",
    "    primary_key = [\"tran_id\"],\n",
    "    partition_key = [\"alert_type\"],         \n",
    "    description = \"alert transactions\",\n",
    "    event_time = ['tran_timestamp'],    \n",
    "    online_enabled = True,                                                \n",
    "    statistics_config = {\"enabled\": True, \"histograms\": True, \"correlations\": True, \"exact_uniqueness\": False}\n",
    ")\n",
    "\n",
    "transaction_labels_fg.insert(transaction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d763696",
   "metadata": {},
   "source": [
    "#### Party feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea962ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_fg = fs.get_or_create_feature_group(\n",
    "    name = \"party_aml_fg\",\n",
    "    version = 1,\n",
    "    primary_key = [\"id\"],\n",
    "    description = \"party fg with labels\",\n",
    "    event_time = ['tran_timestamp'],        \n",
    "    online_enabled = True,\n",
    "    statistics_config = {\"enabled\": True, \"histograms\": True, \"correlations\": True, \"exact_uniqueness\": False}\n",
    ")\n",
    "\n",
    "party_fg.insert(party_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a8294",
   "metadata": {},
   "source": [
    "#### Graph embeddings feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cae0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsfs import engine\n",
    "features = engine.get_instance().parse_schema_feature_group(graph_embdeddings_df)\n",
    "for f in features:\n",
    "    if f.type == \"array<float>\":\n",
    "        f.online_type = \"VARBINARY(200)\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b817de",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embeddings_fg = fs.get_or_create_feature_group(name=\"graph_embeddings_aml_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"id\"],\n",
    "                                       description=\"node embeddings from transactions graph\",\n",
    "                                       event_time = ['tran_timestamp'],      \n",
    "                                       online_enabled=True,                                                \n",
    "                                       statistics_config={\"enabled\": False, \"histograms\": False, \"correlations\": False, \"exact_uniqueness\": False},\n",
    "                                       features=features)\n",
    "\n",
    "graph_embeddings_fg.insert(graph_embdeddings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a17e8c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380618be",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üëì Exploration </span>\n",
    "\n",
    "### Feature groups are now accessible and searchable in the UI\n",
    "![fg-overview](images/fg_explore.gif)\n",
    "\n",
    "## üìä Statistics\n",
    "We can explore feature statistics in the feature groups. If statistics was not enabled when feature group was created then this can be done by:\n",
    "\n",
    "```python\n",
    "transactions_fg = fs.get_or_create_feature_group(\n",
    "    name = \"transactions_monthly_fg\", \n",
    "    version = 1)\n",
    "\n",
    "transactions_fg.statistics_config = {\n",
    "    \"enabled\": True,\n",
    "    \"histograms\": True,\n",
    "    \"correlations\": True\n",
    "}\n",
    "\n",
    "transactions_fg.update_statistics_config()\n",
    "transactions_fg.compute_statistics()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28374315",
   "metadata": {},
   "source": [
    "![fg-stats](images/freature_group_stats.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42707fd",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚è≠Ô∏è **Next:** Part 02 </span>\n",
    "    \n",
    "In the following notebook you will use feature groups to create feature viewa and training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ad91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
